# Stage 3: Final Model Configuration

# Model architecture
model:
  nz: 100              # Size of input noise vector
  ngf: 512             # Number of generator features in first layer
  ndf: 64              # Number of discriminator features in first layer
  nc: 3                # Number of channels (RGB)
  use_spectral_norm: true  # Apply spectral normalization to discriminator (stabilizes training)

# Data configuration
data:
  train_dir: "data/pokemon-dataset-1000/train"  # Path to training data directory
  val_dir: "data/pokemon-dataset-1000/val"      # Path to validation data
  test_dir: "data/pokemon-dataset-1000/test"    # Path to test data (for evaluation)
  num_workers: 4                                # Number of data loading workers
  augment: true                                 # Whether to apply data augmentation (horizontal flips, rotations, color jitter)

# Training configuration
training:
  seed: 42                   # Random seed for reproducibility
  deterministic: false       # Enable deterministic operations (slower)
  batch_size: 64             # Batch size (reduced from 128 - better for spectral norm stability)
  epochs: 200                # Number of training epochs
  lr_g: 0.0002               # Learning rate for generator
  lr_d: 0.0002               # Learning rate for discriminator (equal LR works better with spectral norm)
  beta1: 0.5                 # Beta1 for Adam optimizer
  beta2: 0.999               # Beta2 for Adam optimizer
  loss_type: lsgan           # Loss function: 'bce' or 'lsgan' (LSGAN often more stable)
  label_smoothing: 0.1       # Label smoothing value (0.0 to disable)
  one_sided_label_smoothing: true  # Only smooth real labels (keep fake labels at 0) - more stable
  
  # Training stability options
  d_steps_per_g_step: 2      # Number of discriminator updates per generator update (train D more to balance)
  grad_clip_g: 5.0           # Gradient clipping for generator (reduced from 10.0 - less aggressive)
  grad_clip_d: 5.0           # Gradient clipping for discriminator (added for stability)
  
  # Learning rate scheduler configuration
  lr_scheduler:
    enabled: true             # Enable learning rate decay
    type: step                # Scheduler type: 'step', 'exponential', or 'reduce_on_plateau'
    step_size: 50             # For 'step': decay every N epochs
    gamma: 0.5                # Multiplicative factor for LR decay
    # For 'reduce_on_plateau':
    # mode: min                # 'min' (reduce when metric stops decreasing) or 'max'
    # factor: 0.5              # Factor to reduce LR by
    # patience: 10             # Number of epochs to wait before reducing LR
    # min_lr: 1e-6             # Minimum learning rate
  
  # Early stopping configuration
  early_stopping:
    enabled: true             # Enable early stopping based on FID
    patience: 15              # Increased patience - give more time for improvement
    min_delta: 1.0            # Minimum FID improvement to count (prevents stopping on noise)
  
  # Logging and checkpointing
  log_interval: 50           # Log every N batches
  save_interval: 5           # Save sample images every N epochs
  checkpoint_interval: 25    # Save checkpoint every N epochs
  val_samples: 64            # Number of samples for validation visualization
  
  # Directories
  checkpoint_dir: "checkpoints"
  output_dir: "outputs"
  log_dir: "logs"

